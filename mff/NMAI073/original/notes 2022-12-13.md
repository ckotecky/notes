2022-12-13
lecture #11
-----------


Thm:
----
- `âˆ€n,k: Î£{i âˆˆ [n]} Yi^(m) = k`
- Distribution of `X1^(k), ..., Xn^(k)` is the same as for `Y1^(m), ..., Yn^(m)`  (really different `m,n`)

### Proof:
- `k1 + ... + kn = k`
- `P[X1^(k) = k1, ..., Xn^(k) = kn] = Px =?= Py = P[Y1^(m) = k1, ..., Yn^(m) = kn | Î£Yi = k]`
- `Px = (k  k1, ..., kn) * 1/n^k`
- `Py = P[Y1 = k1, ..., Yn = kn] / P[Î£ Yi = k] = A/B`
	+ `P[Y1 = k1, ..., Yn = kn]` does not have to be written out as `P[Y1 = k1, ..., Yn = kn  âˆ§  Î£ Yi = k]`
	+ whenever `Y1 = k1, ..., Yn = kn` hold, `Î£ Yi = k` is true as well
- `A  =  P[Y1^(m) = k1] * ... * P[Yn^(m) = kn] =  e^{m/n} * (m/n)^k1 / k1! * ... * e^{m/n} * (m/n)^kn / kn!` 
	+ `= e^{-m} * (m/n)^{k1 + ... + kn} * 1 / (k1! * ... * kn!)`
- `B  =  P[Î£ Yi^(m) = k]  =  e^-m * m^k/k!`
	+ `{Î£ Yi^(m) = k} ~ Poi(m/n + ... + m/n) = Poi(m)`
- `A/B  =  k! / k1! * ... * kn! * 1/n^k`


Df:
---
1. `X1^(k), ..., Xn^(k)` are known as the 'exact case'
2. `Y1^(m), ..., Yn^(m)` are called the 'Poisson case'


Thm:
----
1. `f(x1, ..., xn) â‰¥ 0` any function  `=> ð”¼[f(X1^(m), ..., Xn^(m))]  â‰¤  ð”¼[f(Y1^(m), ..., Yn^(m))] * eâˆšm`
2. `ð”¼[f(X1^(m), ..., Xn^(m))]` is monotone in `m  =>  eâˆšm` can be replaced by `2`

### Proof:
1. only
- `ð”¼[f(Y1^(m), ..., Yn^(m))] = Î£{k = 0^âˆž} ð”¼[f(Y1^(m), ..., Yn^(m)) | Î£ Yi^(m) = k] * P[Î£ Yi^(m) = k]`
	+ `Î£{k = 0^âˆž} ð”¼[f(Y1^(m), ..., Yn^(m)) | Î£ Yi^(m) = k] * P[Î£ Yi^(m) = k]  â‰¥  ð”¼[f(Y1^(m), ..., Yn^(m)) | Î£ Yi^(m) = m] * P[Î£ Yi^(m) = m]`
- `= ð”¼[f(X1^(m), ..., Xn^(m))] * P[Î£ Yi^(m) = m]`
	+ by previous theorem
- `P[Î£ Yi^(m) = m]  =  e^-m * m^m/m!`
- `â‰¥  1/(e âˆšm)`
	+ because `m! â‰¤ (m/e)^m * eâˆšm`


Crl:
----
1. Event `A` occurs with probability `â‰¤ p` in the Poisson case (for `Y1^(m), ..., Yn^(m)`)
	`=>`  it occurs with probability `â‰¤ p * eâˆšm` in the Exact case
2. Probability in the Exact case is monotone in `m`
	`=> eâˆšm` can be replace by `2`

### Proof:
- consider `A âŠ† â„^n`
- event in Poisson case `(Y1^(m), ..., Yn^(m)) âˆˆ A`
- event in the exact case `(X1^(m), ..., Yn^(m)) âˆˆ A`
- define function `f = IA: f(x1, ..., xn) = \{`
	- `1` 	`(x1, ..., xn) âˆˆ A`
	- `0` 	otherwise
- we use previous theorem `P[Y1^(m), ...., Yn^(m) âˆˆ A] = ð”¼[f(Y1^(m), ..., Yn^(m))]`
- `P[Y1^(m), ...., Yn^(m) âˆˆ A] = P[f(Y1^(m), ..., Yn^(m)) = 1]`


Thm:
----
- `m = n` and it is large  `=>  P[max load < ln(n) / ln(ln(n))]  â‰¤  1/n`

### Proof:
- by the corollary, we need to show: `P[max(Y1^(m), ..., Yn^(m)) < M]  â‰¤  1 / (e^{âˆšn} * n)`
- Poisson case `max(Y1^(m), ..., Yn^(m))` for `A = [0, M)^n`
- `P[max(Y1^(m), ..., Yn^(m)) < M]  =  P[Y1^(m) < M  âˆ§  ...  âˆ§  Yn^(m) < M]  =  P[Y1^(m) < M] * ... * P[Yn^(m) < M]`
- `Yi ~ Poi(1)`
- `P[Yi^(m) < M]  â‰¤  1 - P[Yi^(m) = M]  =  1 - e^{-1} * 1^n/M!  â‰¤  e^{-1/eM!}`
- `P[Y1^(m) < M] * ... * P[Yn^(m) < M]  â‰¤  (1 - 1/eM!)^n  â‰¤  e^{-n/eM!}`
- `â‰¤  1/n^2`
	+ equivalently `n/eM!  >  2ln(n)`
	+ `M!  â‰¤  M * (M/e)^M`
	+ as excercise
	+ more details will be in the lecture notes
- `M` is the optimizing value to get `ln(n) / ln(ln(n))`





NON-PARAMETRIC STATISTICS
=========================


Ex: Permutation test
--------------------
- random variables `X1, ..., Xn` and `Y1, ..., Yn` that we want to compare
- `H0 =` all are from the same (arbitrary) distribution

- first decide about the statistics `T`
1. `T = {Xm} - {Yn}` 								`{X} = \overline{X} = mean of X1, ..., Xn`
	+ for the one-sided test
	+ either `H0` or `X` is better
2. alternately we can use `T = |{Xm} - {Yn}|` 
	+ for the two-sided test

- decide parameter `Î³`
	+ typical criterion is `P[wrong rejection] < Î± = 0.05`
- if `T > Î³  =>`  reject `H0`

- `P[T > Î³]` assuming `H0`
- `s = {x1, ..., xn, y1, ..., yn}`
- `P[T > Î³(s) | {X1, ... Xn, Y1, ..., Yn} = s] < Î±`

+ still assuming `H0`
+ assume `{X1, X2} = {1, 1}, {Y1, Y2} = 2, 3`
+ `s = {1, 1, 2, 3}` multiset
+ assuming we measure these values, we come up with `Î³(s)`
+ `P[T > Î³(s) | {X1, ... Xn, Y1, ..., Yn} = s]` can be evaluated
+ we have `4!` possibilities of assigning values to `X1, X2, Y1, Y2`
+ for each, we can compute `T`
	* finite probabilistic space `Ï‰ â‰¤ * (m+n  m)`
	- `1, 1 ; 2, 3`   	`-1.5` 	`+1.5`
	- `1, 2 ; 1, 3`		`-0.5` 	`+0.5`
	- `1, 2 ; 3, 1`  	`-0.5`	`+0.5`
	+ sign changes with opposite value assignment