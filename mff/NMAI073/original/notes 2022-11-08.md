2022-11-08
lecture #5
----------



Df: Conditional independence
----------------------------
- given `Î˜ = ğœ—`, the variables `X1, ..., Xn` are independent


Ex: Naive Bayes spam classifier
-------------------------------
- `Î©` = {emails}
- `Ï‰` is one piece of email
- `Î˜` a random variable for `Ï‰ âˆˆ Î© = \{`
	+ `1` 	spam
	+ `2` 	not spam / ham
	+ `3` 	important
	+ ...
- data `Xi = \{`
	+ `1` 	if word `wi` is in `Ï‰`
	+ `0` 	otherwise
+ `i âˆˆ [n]`
1. `n = 1`
	+ `w1 =` "win"
	
	- `P[Î˜ = ğœ— | X1 = x1] = p_Î˜(ğœ—) * p{X1|Î˜}(x1|ğœ—) / Î£{t âˆˆ [2]} (pÎ˜(t) * p{X1|Î˜}(x1|t))`
		+ `p_Î˜(ğœ—)` 	 	prior
			* `â‰ 0.8`  spam
		+ `p{X1|Î˜}(x1|ğœ—)` 		model of emails recieved
2. general `n`
	+ `w2`: bitcoin, `w3`: viagra, `w4`: nigeria, `w5`: late relative...
	
	- `X = (X1, ..., Xn)`
	- `P[Î˜ = ğœ— | X1 = x1, ..., Xn = xn] = p_Î˜(ğœ—) * p{X|Î˜}(x1, ..., xn | ğœ—) / Î£{t âˆˆ [2]} (pÎ˜(t) * p{X1|Î˜}(x1|t))`
		+ `p{X|Î˜}(x1, ..., xn | ğœ—)`
			- joint PMF needs calculate `2^n` combinations of words
			- so we assume **conditional independence**
			- `P[Î˜ = ğœ— | X1 = x1] = pÎ˜(ğœ—) * Ï€{i âˆˆ [n]} p{X|Î˜}(xi|ğœ—) / ...`
			- same as assuming `Ï€{i âˆˆ [n]} p{X|Î˜}(xi|ğœ—) = p{X|Î˜}(x1, ..., xn | ğœ—)`
			+ resulting probabilities may be very small, so logarithm may be used to avoid rounding errors
			
	+ use:
		1. get email 		`Ï‰`
		2. measure data 	`x = x1, ..., xn`
		3. use formula to find `P[Î˜ = 1 | X = x], P[Î˜ = 2 | X = x], ...`
		4. we compare resulting probabilities to see if spam probability is highest etc ...


Ex: Determining bias of a coin
------------------------------
- `P[H] = ğœ— âˆˆ [0,1]`
- assume `ğœ—` is a value of `Î˜` - prior belief of what a coin looks like
	+ we can have a tight normal distribution around `1/2`
	+ or we may be suspicious and believe all probabilities are uniformly distributed
	+ `Î˜` is continuous
	+ PDF `fÎ˜`
+ data `X ~ Bin(n, ğœ—)`
	- toss the coin n times
	- `X =` # of heads
+ conditional PMF for `X` 
	- `p{X|Î˜}(k âˆˆ â„¤, ğœ—) = (n k)ğœ—^k(1-ğœ—)^{n-k}`
+ by **Bayes Thm**
	- `f{Î˜|X}(ğœ—|k) = fÎ˜(ğœ—) * p{X|Î˜}(k|ğœ—) / âˆ«{0^1} fÎ˜(t) p{X|Î˜}(k|t) dt`
		+ `fÎ˜(ğœ—)` 								prior
		+ `âˆ«{0^1} fÎ˜(t) p{X|Î˜}(k|t) dt`     	is analogous to `p(X = k)`  composed of  `p(X = k, Î˜ = t)`
		

Df: Beta distribution
---------------------
- parameters `Î±,Î² â‰¥ 1`
- `ğœ— âˆˆ [0,1]`
	+ otherwise `fÎ˜(ğœ—) = 0`
- density `fÎ˜(ğœ—) = ğœ—^{Î± - 1} * (1 - ğœ—){Î² - 1} / B(Î±, Î²)`
	+ `B(Î±, Î²)` **beta function**
		* constant `âˆ€ Î±,Î²`
	+ `B(1,1) = 1` 	 		`X ~ U(0,1)`
	+ `B(1,2) = 1/2` 		`X ~ 1 \{`
							- `0`  		`x < 0 or 1 < x`
							- `1 - x` 	otherwise
	+ generarly normal? 
	+ max at `(Î± - 1)/(Î± + Î² - 2) =` **"mode"**
	+ `B(Î±,Î²) = (Î± - 1)! * (Î² - 1)! / (Î± + Î² - 2)! = 1 / (Î±+Î²-2 Î±-1)`
- `ğ”¼[Î˜] = Î± / (Î± + Î²) =` **"mean"**
+ by **Bayes thm**?
	+ `f{Î˜|X}(ğœ—|k) = c1 * ğœ—^{Î± - 1} * (1 - ğœ—)^{Î² - 1} * c2 * ğœ—^k * (1 - ğœ—)^{n - k} * c3`
		- `c1, c2, c3` constants independent of `ğœ—`
		- = `c_t? ğœ—^{Î± + k - 1} (1 - ğœ—)^{Î² + n - k - 1}`
		- = beta distribution for `Î±' = Î± + k, Î²' = Î² + n - k`
1. if we start with `Î± = Î² = 1` (flat prior)
	- `f{Î˜|k}(ğœ—|k) = ğœ—^k (1 - ğœ—)^{1 - k} / B(k + 1, n - k + 1)`
		+ = `fÎ˜(ğœ—) * p{X|Î˜}(k|ğœ—)` / const
	- if we want to do some point estimate, we have two possibilities
		1. MAP (maximum aposterior)
			- `{{ğœ—}} = k/n`												`{{ğœ—}} is \hat{ğœ—}`
			- same as maximum likelihood estimate
				+ `pX(k;ğœ—)`
		2. LMS (least mean square estimate)
			- `{{ğœ—}} = ğ”¼[Î˜ | X = k] = (k + 1) / (n + 2)`





Normal random vairables
-----------------------
- `X = (X1, ..., Xn)`
- `Xi ~ N(ğœ—, Ïƒ_i^2)`
- `Xi` are independent `âˆ€ ğœ—`
+ `Xi = Î˜ + Wi`
	* `Wi ~ N(0, Ïƒ_i^2)`  	error of ith measurement
	* `Î˜` 					true value
+ `ğœ—` is a value of `Î˜ ~ N(x0, Ïƒ_0^2)`
+ `fÎ˜(ğœ—) = c1 * e^{-(ğœ— - x0)^2 / (2 * Ïƒ0^2)}`
+ `f{X|Î˜}(x|ğœ—) = c2 * Ï€{i âˆˆ [n]} e^{- (ğœ— - xi)^2 / (2 * Ïƒ^2)}`
+ `f{Î˜|X}(ğœ—|x) = fÎ˜(ğœ—) * f{X|Î˜}(x|ğœ—) / âˆ«t fÎ˜(t) * f{X|Î˜}(x|t) dt = c3 * c1 * c2 Ï€{i âˆˆ [n]0} e^{- (ğœ— - xi)^2 / (2 * Ïƒ^2)} = c4 * e^{- Î£{i âˆˆ [n]0} (ğœ— - x)^2 / (2 * Ïƒ^2)}`
	* `= - ((ğœ— - x0)^2 / (2 * Ïƒ0^2) + (ğœ— - x1)^2 / (2 * Ïƒ1^2) + ...)`
	* `= - (ğœ—^2 (1/(2Ïƒ_0^2) + 1/(2Ïƒ_1^2) + ...))	`
		- `(1/(2Ïƒ_0^2) + 1/(2Ïƒ_1^2) + ...) = 1/2v`
	* `v = 1 / (Î£{i âˆˆ [n]0} 1/Ïƒ_i^2)`
	* ... `= -  (ğœ—^2 (1/(2Ïƒ_0^2) + 1/(2Ïƒ_1^2)) - 2ğœ—(x0/2Ïƒ_0^2 + x1/2Ïƒ_1^2 + ...) + c5)`
	* `= - ((ğœ— - c6)^2 / 2v)`
	- `A = Î£ xi/Ïƒ_i^2`
	- `m = A/v`
		+ general average
* we stared with `Î˜ ~ N(x0, Ïƒ_0^2)`
* we ended up with updated `{Î˜ | X = x} ~ N(m, v)`
- special case `Ïƒ0 = Ïƒ1 = ... Ïƒ`
	- `v = Ïƒ^2 / (n + 1)`
	- `m = (Î£ xi) / (n + 1)`
	+ classically there is no prior
		- `m = {x} = Î£xi / n`


Conjugate prior
---------------
1. Bin data 
	+ good to have Beta prior
	+ after updating we again get a Beta posterior
2. Normal data
	+ we should use Normal prior
	+ we get a Normal posterior