2022-11-08
lecture #5
----------



Df: Conditional independence
----------------------------
- given `Θ = 𝜗`, the variables `X1, ..., Xn` are independent


Ex: Naive Bayes spam classifier
-------------------------------
- `Ω` = {emails}
- `ω` is one piece of email
- `Θ` a random variable for `ω ∈ Ω = \{`
	+ `1` 	spam
	+ `2` 	not spam / ham
	+ `3` 	important
	+ ...
- data `Xi = \{`
	+ `1` 	if word `wi` is in `ω`
	+ `0` 	otherwise
+ `i ∈ [n]`
1. `n = 1`
	+ `w1 =` "win"
	
	- `P[Θ = 𝜗 | X1 = x1] = p_Θ(𝜗) * p{X1|Θ}(x1|𝜗) / Σ{t ∈ [2]} (pΘ(t) * p{X1|Θ}(x1|t))`
		+ `p_Θ(𝜗)` 	 	prior
			* `≐ 0.8`  spam
		+ `p{X1|Θ}(x1|𝜗)` 		model of emails recieved
2. general `n`
	+ `w2`: bitcoin, `w3`: viagra, `w4`: nigeria, `w5`: late relative...
	
	- `X = (X1, ..., Xn)`
	- `P[Θ = 𝜗 | X1 = x1, ..., Xn = xn] = p_Θ(𝜗) * p{X|Θ}(x1, ..., xn | 𝜗) / Σ{t ∈ [2]} (pΘ(t) * p{X1|Θ}(x1|t))`
		+ `p{X|Θ}(x1, ..., xn | 𝜗)`
			- joint PMF needs calculate `2^n` combinations of words
			- so we assume **conditional independence**
			- `P[Θ = 𝜗 | X1 = x1] = pΘ(𝜗) * π{i ∈ [n]} p{X|Θ}(xi|𝜗) / ...`
			- same as assuming `π{i ∈ [n]} p{X|Θ}(xi|𝜗) = p{X|Θ}(x1, ..., xn | 𝜗)`
			+ resulting probabilities may be very small, so logarithm may be used to avoid rounding errors
			
	+ use:
		1. get email 		`ω`
		2. measure data 	`x = x1, ..., xn`
		3. use formula to find `P[Θ = 1 | X = x], P[Θ = 2 | X = x], ...`
		4. we compare resulting probabilities to see if spam probability is highest etc ...


Ex: Determining bias of a coin
------------------------------
- `P[H] = 𝜗 ∈ [0,1]`
- assume `𝜗` is a value of `Θ` - prior belief of what a coin looks like
	+ we can have a tight normal distribution around `1/2`
	+ or we may be suspicious and believe all probabilities are uniformly distributed
	+ `Θ` is continuous
	+ PDF `fΘ`
+ data `X ~ Bin(n, 𝜗)`
	- toss the coin n times
	- `X =` # of heads
+ conditional PMF for `X` 
	- `p{X|Θ}(k ∈ ℤ, 𝜗) = (n k)𝜗^k(1-𝜗)^{n-k}`
+ by **Bayes Thm**
	- `f{Θ|X}(𝜗|k) = fΘ(𝜗) * p{X|Θ}(k|𝜗) / ∫{0^1} fΘ(t) p{X|Θ}(k|t) dt`
		+ `fΘ(𝜗)` 								prior
		+ `∫{0^1} fΘ(t) p{X|Θ}(k|t) dt`     	is analogous to `p(X = k)`  composed of  `p(X = k, Θ = t)`
		

Df: Beta distribution
---------------------
- parameters `α,β ≥ 1`
- `𝜗 ∈ [0,1]`
	+ otherwise `fΘ(𝜗) = 0`
- density `fΘ(𝜗) = 𝜗^{α - 1} * (1 - 𝜗){β - 1} / B(α, β)`
	+ `B(α, β)` **beta function**
		* constant `∀ α,β`
	+ `B(1,1) = 1` 	 		`X ~ U(0,1)`
	+ `B(1,2) = 1/2` 		`X ~ 1 \{`
							- `0`  		`x < 0 or 1 < x`
							- `1 - x` 	otherwise
	+ generarly normal? 
	+ max at `(α - 1)/(α + β - 2) =` **"mode"**
	+ `B(α,β) = (α - 1)! * (β - 1)! / (α + β - 2)! = 1 / (α+β-2 α-1)`
- `𝔼[Θ] = α / (α + β) =` **"mean"**
+ by **Bayes thm**?
	+ `f{Θ|X}(𝜗|k) = c1 * 𝜗^{α - 1} * (1 - 𝜗)^{β - 1} * c2 * 𝜗^k * (1 - 𝜗)^{n - k} * c3`
		- `c1, c2, c3` constants independent of `𝜗`
		- = `c_t? 𝜗^{α + k - 1} (1 - 𝜗)^{β + n - k - 1}`
		- = beta distribution for `α' = α + k, β' = β + n - k`
1. if we start with `α = β = 1` (flat prior)
	- `f{Θ|k}(𝜗|k) = 𝜗^k (1 - 𝜗)^{1 - k} / B(k + 1, n - k + 1)`
		+ = `fΘ(𝜗) * p{X|Θ}(k|𝜗)` / const
	- if we want to do some point estimate, we have two possibilities
		1. MAP (maximum aposterior)
			- `{{𝜗}} = k/n`												`{{𝜗}} is \hat{𝜗}`
			- same as maximum likelihood estimate
				+ `pX(k;𝜗)`
		2. LMS (least mean square estimate)
			- `{{𝜗}} = 𝔼[Θ | X = k] = (k + 1) / (n + 2)`





Normal random vairables
-----------------------
- `X = (X1, ..., Xn)`
- `Xi ~ N(𝜗, σ_i^2)`
- `Xi` are independent `∀ 𝜗`
+ `Xi = Θ + Wi`
	* `Wi ~ N(0, σ_i^2)`  	error of ith measurement
	* `Θ` 					true value
+ `𝜗` is a value of `Θ ~ N(x0, σ_0^2)`
+ `fΘ(𝜗) = c1 * e^{-(𝜗 - x0)^2 / (2 * σ0^2)}`
+ `f{X|Θ}(x|𝜗) = c2 * π{i ∈ [n]} e^{- (𝜗 - xi)^2 / (2 * σ^2)}`
+ `f{Θ|X}(𝜗|x) = fΘ(𝜗) * f{X|Θ}(x|𝜗) / ∫t fΘ(t) * f{X|Θ}(x|t) dt = c3 * c1 * c2 π{i ∈ [n]0} e^{- (𝜗 - xi)^2 / (2 * σ^2)} = c4 * e^{- Σ{i ∈ [n]0} (𝜗 - x)^2 / (2 * σ^2)}`
	* `= - ((𝜗 - x0)^2 / (2 * σ0^2) + (𝜗 - x1)^2 / (2 * σ1^2) + ...)`
	* `= - (𝜗^2 (1/(2σ_0^2) + 1/(2σ_1^2) + ...))	`
		- `(1/(2σ_0^2) + 1/(2σ_1^2) + ...) = 1/2v`
	* `v = 1 / (Σ{i ∈ [n]0} 1/σ_i^2)`
	* ... `= -  (𝜗^2 (1/(2σ_0^2) + 1/(2σ_1^2)) - 2𝜗(x0/2σ_0^2 + x1/2σ_1^2 + ...) + c5)`
	* `= - ((𝜗 - c6)^2 / 2v)`
	- `A = Σ xi/σ_i^2`
	- `m = A/v`
		+ general average
* we stared with `Θ ~ N(x0, σ_0^2)`
* we ended up with updated `{Θ | X = x} ~ N(m, v)`
- special case `σ0 = σ1 = ... σ`
	- `v = σ^2 / (n + 1)`
	- `m = (Σ xi) / (n + 1)`
	+ classically there is no prior
		- `m = {x} = Σxi / n`


Conjugate prior
---------------
1. Bin data 
	+ good to have Beta prior
	+ after updating we again get a Beta posterior
2. Normal data
	+ we should use Normal prior
	+ we get a Normal posterior