2022-11-22
lecture #7
----------



STOCHASTIC PROCESSES
====================
- a sequence of random variables `X1, X2, X3, ...`

1. Markov chain
2. Wiener process
	- used to study:
		+ Brownian motion
		+ stock prices
	- limit version of random walks
3. arrival times
	- waiting for success
	- `0 < T1 < T2 < ...` random variables
	


Df: Bernoulli process
---------------------
- `Bp(p)`
1. `X1, X2, ...` independent random variables `~ Ber(p)`
	- `T = min{t: Xt = 1}` time of first succes / arrival
	- `T ~ Geom(p)`
2. `Tk =` time of `k`-th success
	- `Tk = min{t: X1 + ... + Xt = k}`
	- `T1 = T`
3. `Lk = k-th` waiting / inter-arrival time
	- `Lk = Tk - T{k-1} where T0 = 0`
	- `Lk ~ L1 = T  =>  Lk ~ Geom(p)`


Ob: Bernoulli process
---------------------
1. `Xn, X{n+1}, ... is` also `Bp(p)`
2. `XN, X{N+1}, ... is` also `Bp(p)`
	- with `N` random variable depending only on the past 
	- for example `N = min{t: X1 + ... + Xt = 10}`
3. restarting the `Bp` at time `N = T{k+1}` gives another `Bp(p)`
4. `L1, L2, ...` are independent
5. `Tk = L1 + ... + Lk`
6. `𝔼[Tk] = 𝔼[L1] + ... + 𝔼[Lk] = k/p`
7. `var[Tk] = var[L1] + ... + var[Lk] = k * (1-p) / p^2`
8. `P[Tk = t]`
	- we can use the convolution formula
	- `(t-1 k-1) * p^k * (1-p)^{t-k}`
		+ time to `k`-th success
		+ **Pascal's distribution** of `k`-th success / **negative binomial distribution**


Ex: Bernoulli process
---------------------

| `t` | 1 | 2 | 3 | 4 | 5 | 6 |
|---|---|---|---|---|---|---|
| `Xt`| 0 | 1 | 1 | 0 | 0 | 1 |

- `k = 3, t = 6`
- `P[T3 = 6]`
	+ `k-1` ones at positions `1, ..., t-1`
	+ `= (t-1 k-1) * p^k * (1-p)^{t-k}`
		* `p^k` - ones at positions of success
		* `(1-p)^{t-k}` - zeroes at other times


Df: Bernoulli process alternative definition
--------------------------------------------
- `Nt = X1 + ... + Xt` - # of successes till time `t ~ Bin(t, p)`
	+ `𝔼[Nt] = tp`
	+ `Var[Nt] = tp(1-p)`
	+ `P[Nt = k] = P_Nt(k) = (t k) * p^k & (1-p)^{t-k}`
- `L1, L2, ...` independent `~ Geom(p)`
- `Tk = L1 + ... + Lk`
- `Xi = \{`
	- `1 	Tk = i`
	- `0 	otherwise`
	+ `(Xi) ~ Bp(p)`
	

Ex: Bernoulli process alternative definition
--------------------------------------------
- time till next rain `~ Geom(p)`
- what is `P[it rains at time 10 & 20] = P[X10 = 1] * P[X20 = 1] = p^2 = P[X10 = X20 = 1]`


Df: Merging
-----------
- `X1, X2, ...  ~ Bp(p)`
- `Y1, Y2, ...  ~ Bp(p)`
- `(Xi), (Yi)` independent
- `Zi = Xi ∨ Yi`
- `Z1, Z2, ...  ~ Bp(p + q - pq)`
- `P[Zi = 1] = P[Xi = 1 ∨ Yi = 1] = `


Ex: Merging
-----------
- `Xi = 1`  	it rains at day `i`
- `Yi = 1` 		it snows at day `i`
- `Zi = 1` 		bad weather


Df: Splitting
-------------
- `Z1, Z2, ...  ~ Bp(r)`
- if `Zi = 1`, then `Xi = \{`
	- 1 	probability `α`
	- 0 	probability `1 - α`
- `Zi = 0  =>  Xi = 0`
- assuming everything is independent
- `X1, X2, ...  ~ Bp(αr)`


Df: Poisson process
-------------------
- continuous approximation of Bp

+ `Pp(λ)`
+ `λ` = intensity paramter
+ arrival times `0 < T1 < T2 < ... ∈ ℝ`
1. `P[k, τ] = P[k arrivals]` is same `∀` interval of length `τ`
2. .# of arrivals in interval `[a, b]` is independent of # in `[0, a)`
3. Conditions:
	- `P[0, τ] = 1 - λτ + o(1)`
	- `P[1, τ] = λτ + o(1)`
	- `P[k, τ] = o(1)`		for `k ≥ 2`
	- `o(1)` is for `τ --> 0`
4. `Tk` - time of `k`-th arrival
5. `Nt =` # of arrivals in `[0, t] ~ Pois(λt)`
	- `𝔼[Nt] = λt`
	- `var[Nt] = λt`
6. `P[k, t] = e^{-λt} * (λt)^k/k!`
7. `Lk = Tk - T{k-1}`
	- `P[Lk ≥ t] = P[no arrival in [T{k-1}, T{k-1} + t]] = P[0, t] = e^{-λt}`
	- `P[Lk ≤ t] = 1 - e^{-λt}`
	- `=> Lk ~ Exp(λ)`


Ob: Deriving above distribution
-------------------------------
- we know that in `Pp(λ): P[Nt = k] = P[k, t]`

- interval length `t`
- large `l` number of subintrvals
- length of one `= t/l`
+ `P[1, t/l] = λt/l + o(1)` - probability of one arrival
- `P[Nt = k] = P[k, t] ≐ P[k of the subintervals see 1 arrival] = P[Bin(l, λt/l) = k]`
	+ `P[Bin(l, λt/l) = k] --{l -> ∞}--> Pois(λt)`
- `P[k, t] = e^{-λt} * (λt)^k/k!`


Ex: Poisson process
-------------------
- arrival times of emails
- assume, that normally we get 10 emails per day
- what is the probability we get 0 emails in 24 after the last one
+ time in hours
+ `𝔼[N_24] = 24 * λ = 10  =>  λ = 10/24`
+ `P[Lk ≥ 24] = e^{-λ * 24} = e^{-10}`
* `P[no email in [0,1] ∪ [2,5]] = P[0,1] * P[0,3] = e^{-λ} * e^{-3λ} = e^{-4λ} = e^{-10/6}`


Df: Poisson process alternative definition
------------------------------------------
- `L1, L2, ...` independent and `i? ~ Exp(λ)`
- `Tk = L1 + ... + Lk`
- `Nt =` # of arrivals in `[0, t] ~ Pois(λt)`


Thm:
----
- alternative definition of the Poisson process is still a Poisson process
- `N{ti} - N{t{i-1}}` are independent `~ Poi(λ(ti - t{i-1}))`