2023-01-03
lecture #13
-----------


Df: Moment generating function
------------------------------
1. `X` is a random variable
2. `s ∈ ℝ`
	`=> M_X(s) = 𝔼[e^{sX}]`


Ex:
---
- `X ~ Ber(p)`
- `P[X = 0] = 1 - p`
- `P[X = 1] = p`
- `𝔼[e^{sX}] = pe^{s1} + (1 - p)e^{s0} = (1 - p) + pe^s = 1 + ps + (ps^2)/2! + (ps^3)/3! + ...`
	+ `e^s = Σ{k = 0}^∞ s^k/k!`
- `p = 𝔼[X] = 𝔼[X^k]` for `k = 1`
- `𝔼[X^0] = 1`


Th:
---
- `s ∈` interval, where `M_X(s)` is defined and finite
	`=>  M_X(s) = Σ{k = 0}^∞ 𝔼[X^k] * s^k/k!`

+ `𝔼[X^k]` is called the `k`-th moment
- `k = 1  =>  𝔼[X]`
- `k = 2  =>  𝔼[X^2] = var[X] + 𝔼[X]^2`

### Proof:
- finite: `M_X(s) = 𝔼[e^{sX}] = 𝔼[Σ{k = 0}^∞ (sX)^k/k!]`
- inifinite: further terms begome negligible
- `=?= 𝔼[(sX)^k/k!]`


Ob:
---
- `∀X: M_X(0) = 1`


Ex:
---
- `Y ~ Exp(λ)`
- `M_Y(s) = 𝔼[e^{sY}] =LOTUS= ∫{-∞^∞} e^{sy} fY(y) dy = ∫{0^∞} e^{sy} λe^{-λy} dy = λ [e^{(s - λ)y}/(s - λ)]{0^∞} = \{`
	- `∞`						`s ≥ λ`
	- `1/(1 - s/λ)` 			`s < λ`
		- `λ(0 - 1/(s - λ)) = λ/(λ - s) = 1/(1 - s/λ) = 1 + s/λ + (s/λ)^2 + (s/λ)^3 + ...`
- `𝔼[Y^k] = k!/λ^k`
	

Ex:
---
- `Z ~ N(0, 1)`
- `M_Z(s) = 𝔼[e^{sZ}] = ∫{-∞^∞} e^{sz} φ(z) dz = ∫{-∞^∞} 1/√{2π} e^{-z^2/2} e^{sz} dz = ∫{-∞^∞} 1/√{2π} e^{-(z - s)^2/2} e^{s^2/2} dz = e^{s^2/2}`
	- `= ∫{-∞^∞} 1/√{2π} e^{-(z - s)^2/2} dz = 1`
- `e^{s^2/2} = 1 + s^2/2 + (s^2/2)^2/2! + (s^2/2)^3/3! + ...`
- `𝔼[Z] = 0`
- `var[Z] = 𝔼[Z^2] = 1`
- `𝔼[Z^3] = 0`
- `𝔼[Z^3] = 4!/(2 * 2 * 2) = 3`


Th:
---
- `M_{aX + b}(s) = e^{bs} * M_X(as)`

### Proof:
- `𝔼[e^{aX + b}] = 𝔼[e^{(as)X} * e^{sb}]`


Thm:
----
- `X,Y` independent  `=>  M_{X + Y} = M_X * M_Y`

### Proof:
- `𝔼[e^{s(X + Y)}] = 𝔼[e^{sX} * e^{sY}] = 𝔼[e^{sX}] * 𝔼[e^{sY}] = M_X * M_Y`
	- `X,Y` are independent  `=>  e^{sX}, e^{sY}` are independent


Thm:
----
- `∃ε > 0: ∀s ∈ (-ε, ε): M_X(s) = M_Y(s) ∈ ℝ`
	`=>  ∀t ∈ ℝ: F_X(t) = F_Y(t)`


Thm:
----
1. `∃ε > 0: ∀s ∈ (-ε, ε): M_{Yn}(s) -> M_Z(s)`
2. `F_Z` is continuous
	`=>  F_{Yn} -> F_Z`
	`=>  Y_n -d-> Z`

- `∀t ∈ ℝ: lim{n -> ∞} P[Yn ≤ t] = P[Z ≤ t]`


Thm: Central limit theorem
--------------------------
1. `X1, X2, ...` independently identically distributed
2. `𝔼[Xi] = μ`
3. `var[Xi] = σ^2`
4. `Yn = (X1 + ... + Xn - nμ) / (σ√n)`
	`=>  Yn -d-> N(0,1)`

### Proof:
- we will use previous theorem
- `Z ~ N(0, 1)`
- `M_Z = e^{?s^2/2}`
- `F_Z` continuous
- assume `μ = 0`
	- otherwise `Xi' = Xi - μ`
	- `𝔼[Xi'] = μ' = 0`
- assume `M_{Xi}(s)` exists
- `Yn = Σ Xi / (σ √n)`
- `M_{Yn}(s) = M_{Σ Xi} s/(σ√n) = M_{X1} (s/(σ√n))^n`
	- by previous theorems
- `M_{X1}(s) = Σ 𝔼[X1^k] * s^k/k! = 1 + 0s + σ^2 * s^2/2 + o(s^2)`
	- `σ^2 = 𝔼[X1^2]`
- `= (1 + σ^2 s^2/(2 * σ^2 * n) + o(s^2/(σ^2 * n)))^n`
	- `(1 + σ^2 s^2/(2 * σ^2 * n) = h(s)`
	- `log(h(s)) = n * log(1 + s^2/2n + o(s^2/2n))`
	- `(n * log(1 + s^2/2n + o(s^2))) / (s^2/2n + o(s^2/2n)`
	- `log(1 + t)/t -{t -> 0}-> 1`
	- `s^2/2n --> 0`
	- `log h(s) --> s^2/2`
- `≐ (1 + (s^2/2)/n)^n --> e^{s^2/2} = M_Z(s)`
- `=>  Yn -d-> Z`


Th: Chernoff
------------
- `X1, ..., Xn = ±1` independent, identically distributed, with probability `1/2`
- `X = Σ Xi`
- `σ^2 = var[X] = n`
- `t > 0`
	`=>  P[X ≤ t] = P[X ≥ t] ≤ e^{-t^2/(2σ^2)}`

### Proof:
- Markov inequality for `s > 0: P[e^{sX} ≥ e^{st}] ≤ 𝔼[e^{sX}] / e^{st} ≤ e^{ns^2/2 - st}`
- `𝔼[e^{sX}] = M_X(s) = M_{X1}(s) * ... * M_{Xn}(s) = (e^s/2 + e^{-s}/2)^n ≤ e^{ns^2/2}`
- `e^s/2 + e^{-s}/2 = 1/2 * (1 + s + s^2/2 + s^3/3! + s^4/4! ...) + 1/2 * (1 - s + s^2/2 - s^3/3! + s^4/4!`
- `= Σ{k = 0^∞} s^{2k}/(2k)! ≤ Σ (s^2/2)^k/k! = e^{s^2/2}`
- `1/(2k)! ≤ 1/(2^k k!)`
- `𝔼[e^{sX}] / e^{st} ≤ e^{s^2/2}`
- `e^{-t^2/(2σ^2)} = e^{-t^2/2n}`
- `s = t/n`


INFORMATION THEORY
==================


Ex:
---
- data (book, DNA, binary file)
- what is the informational content?
- model:
	- `X1, ..., Xn` independent identically distributed random variables over a finite alphabet `A`
	- `∀i ∈ ℕ, ∀a ∈ A: p(a) = P[Xi = a]`
	- `p = (p(a)){a ∈ A}`
- `A^n` possible words
- `C_n ⊆ A^n` contains typical words
- the goal is to encode `C_n` with a small amount of bits
- `{Xn} = (X1, ..., Xn)` 								`{Xn} = \vec{Xn}`
- `L(n, ε) = min{l: ∃ C_n ⊂ A^n: |Cn| ≤ 2^l}`
	- `|Cn| ≤ 2^l` we can encode using `l` bits
	- `P[{Xn} ∈ Cn] ≥ 1 - ε`


Df: Entropy function
--------------------
- `H(p) = Σ{a ∈ A} -p(a) * log2(p(a))`


Th: Source coding theorem (Shannon: 1948)
-----------------------------------------
- `∀ε > 0: lim{n -> ∞} L(n, ε)/n = H(p)`
- entropy of `p` or entropy of `Xi`

### Proof:
1. `≤`
	+ `Yi = log2(1/p(Xi))`
	+ `𝔼[Yi] = Σ{a ∈ A} P[Xi = a] 𝔼[Yi | Xi = a] = Σ{a ∈ A} -p(a) * log2(p(a))`
	+ `Y1, Y2, ...` infinite sequence of independent, identically distributed random variables
	+ `A` is finite  `=>   var[Yi]` is finite  `=>`  we can use the weak law of large numbers
	
	+ `Sn = Σ Yi / n`
	+ by the weak law: `P[|Sn - 𝔼[Sn]| > δ| -> 0`
	+ `𝔼[Sn] = 𝔼[Yi] = H(p)`
	+ `< δ` if `n` is large
	+ `Sn = -1/n Σ{n = 1^n} log2(p(Xi)) = -1/n log2(π p(Xi)) =independence= -1/n log2(p^*({Xn}))` for `p^*` probability on `n`-tuples
	+ `p^*({Xn}) = 2^{-n * Sn}`
	+ `p^*({Xn}) ≐ 2^{-n * Sn}` with high probability
	+ `Bn = \{w ∈ A^n: 2^{-n * (H(p) + δ)} ≤ p^*(w) ≤ 2^{-n * (H(p) - δ)}\}`
	+ if `0 < δ < ε`, then we can use `Cn = Bn`
		* `P[Bn] ≥ 1 - δ ≥ 1 - ε`
		* `1 = Σ{w ∈ A^n} p^*(w) ≥ Σ{w ∈ Bn} p^*(w) ≥ |Bn| * 2^{-n(H(p) + δ)}`
		* `|Bn| ≤ 2^{n(H(p) + δ)}`
		* `L(n, ε) ≤ n(H(p) + δ)`