2023-01-03
lecture #13
-----------


Df: Moment generating function
------------------------------
1. `X` is a random variable
2. `s âˆˆ â„`
	`=> M_X(s) = ğ”¼[e^{sX}]`


Ex:
---
- `X ~ Ber(p)`
- `P[X = 0] = 1 - p`
- `P[X = 1] = p`
- `ğ”¼[e^{sX}] = pe^{s1} + (1 - p)e^{s0} = (1 - p) + pe^s = 1 + ps + (ps^2)/2! + (ps^3)/3! + ...`
	+ `e^s = Î£{k = 0}^âˆ s^k/k!`
- `p = ğ”¼[X] = ğ”¼[X^k]` for `k = 1`
- `ğ”¼[X^0] = 1`


Th:
---
- `s âˆˆ` interval, where `M_X(s)` is defined and finite
	`=>  M_X(s) = Î£{k = 0}^âˆ ğ”¼[X^k] * s^k/k!`

+ `ğ”¼[X^k]` is called the `k`-th moment
- `k = 1  =>  ğ”¼[X]`
- `k = 2  =>  ğ”¼[X^2] = var[X] + ğ”¼[X]^2`

### Proof:
- finite: `M_X(s) = ğ”¼[e^{sX}] = ğ”¼[Î£{k = 0}^âˆ (sX)^k/k!]`
- inifinite: further terms begome negligible
- `=?= ğ”¼[(sX)^k/k!]`


Ob:
---
- `âˆ€X: M_X(0) = 1`


Ex:
---
- `Y ~ Exp(Î»)`
- `M_Y(s) = ğ”¼[e^{sY}] =LOTUS= âˆ«{-âˆ^âˆ} e^{sy} fY(y) dy = âˆ«{0^âˆ} e^{sy} Î»e^{-Î»y} dy = Î» [e^{(s - Î»)y}/(s - Î»)]{0^âˆ} = \{`
	- `âˆ`						`s â‰¥ Î»`
	- `1/(1 - s/Î»)` 			`s < Î»`
		- `Î»(0 - 1/(s - Î»)) = Î»/(Î» - s) = 1/(1 - s/Î») = 1 + s/Î» + (s/Î»)^2 + (s/Î»)^3 + ...`
- `ğ”¼[Y^k] = k!/Î»^k`
	

Ex:
---
- `Z ~ N(0, 1)`
- `M_Z(s) = ğ”¼[e^{sZ}] = âˆ«{-âˆ^âˆ} e^{sz} Ï†(z) dz = âˆ«{-âˆ^âˆ} 1/âˆš{2Ï€} e^{-z^2/2} e^{sz} dz = âˆ«{-âˆ^âˆ} 1/âˆš{2Ï€} e^{-(z - s)^2/2} e^{s^2/2} dz = e^{s^2/2}`
	- `= âˆ«{-âˆ^âˆ} 1/âˆš{2Ï€} e^{-(z - s)^2/2} dz = 1`
- `e^{s^2/2} = 1 + s^2/2 + (s^2/2)^2/2! + (s^2/2)^3/3! + ...`
- `ğ”¼[Z] = 0`
- `var[Z] = ğ”¼[Z^2] = 1`
- `ğ”¼[Z^3] = 0`
- `ğ”¼[Z^3] = 4!/(2 * 2 * 2) = 3`


Th:
---
- `M_{aX + b}(s) = e^{bs} * M_X(as)`

### Proof:
- `ğ”¼[e^{aX + b}] = ğ”¼[e^{(as)X} * e^{sb}]`


Thm:
----
- `X,Y` independent  `=>  M_{X + Y} = M_X * M_Y`

### Proof:
- `ğ”¼[e^{s(X + Y)}] = ğ”¼[e^{sX} * e^{sY}] = ğ”¼[e^{sX}] * ğ”¼[e^{sY}] = M_X * M_Y`
	- `X,Y` are independent  `=>  e^{sX}, e^{sY}` are independent


Thm:
----
- `âˆƒÎµ > 0: âˆ€s âˆˆ (-Îµ, Îµ): M_X(s) = M_Y(s) âˆˆ â„`
	`=>  âˆ€t âˆˆ â„: F_X(t) = F_Y(t)`


Thm:
----
1. `âˆƒÎµ > 0: âˆ€s âˆˆ (-Îµ, Îµ): M_{Yn}(s) -> M_Z(s)`
2. `F_Z` is continuous
	`=>  F_{Yn} -> F_Z`
	`=>  Y_n -d-> Z`

- `âˆ€t âˆˆ â„: lim{n -> âˆ} P[Yn â‰¤ t] = P[Z â‰¤ t]`


Thm: Central limit theorem
--------------------------
1. `X1, X2, ...` independently identically distributed
2. `ğ”¼[Xi] = Î¼`
3. `var[Xi] = Ïƒ^2`
4. `Yn = (X1 + ... + Xn - nÎ¼) / (Ïƒâˆšn)`
	`=>  Yn -d-> N(0,1)`

### Proof:
- we will use previous theorem
- `Z ~ N(0, 1)`
- `M_Z = e^{?s^2/2}`
- `F_Z` continuous
- assume `Î¼ = 0`
	- otherwise `Xi' = Xi - Î¼`
	- `ğ”¼[Xi'] = Î¼' = 0`
- assume `M_{Xi}(s)` exists
- `Yn = Î£ Xi / (Ïƒ âˆšn)`
- `M_{Yn}(s) = M_{Î£ Xi} s/(Ïƒâˆšn) = M_{X1} (s/(Ïƒâˆšn))^n`
	- by previous theorems
- `M_{X1}(s) = Î£ ğ”¼[X1^k] * s^k/k! = 1 + 0s + Ïƒ^2 * s^2/2 + o(s^2)`
	- `Ïƒ^2 = ğ”¼[X1^2]`
- `= (1 + Ïƒ^2 s^2/(2 * Ïƒ^2 * n) + o(s^2/(Ïƒ^2 * n)))^n`
	- `(1 + Ïƒ^2 s^2/(2 * Ïƒ^2 * n) = h(s)`
	- `log(h(s)) = n * log(1 + s^2/2n + o(s^2/2n))`
	- `(n * log(1 + s^2/2n + o(s^2))) / (s^2/2n + o(s^2/2n)`
	- `log(1 + t)/t -{t -> 0}-> 1`
	- `s^2/2n --> 0`
	- `log h(s) --> s^2/2`
- `â‰ (1 + (s^2/2)/n)^n --> e^{s^2/2} = M_Z(s)`
- `=>  Yn -d-> Z`


Th: Chernoff
------------
- `X1, ..., Xn = Â±1` independent, identically distributed, with probability `1/2`
- `X = Î£ Xi`
- `Ïƒ^2 = var[X] = n`
- `t > 0`
	`=>  P[X â‰¤ t] = P[X â‰¥ t] â‰¤ e^{-t^2/(2Ïƒ^2)}`

### Proof:
- Markov inequality for `s > 0: P[e^{sX} â‰¥ e^{st}] â‰¤ ğ”¼[e^{sX}] / e^{st} â‰¤ e^{ns^2/2 - st}`
- `ğ”¼[e^{sX}] = M_X(s) = M_{X1}(s) * ... * M_{Xn}(s) = (e^s/2 + e^{-s}/2)^n â‰¤ e^{ns^2/2}`
- `e^s/2 + e^{-s}/2 = 1/2 * (1 + s + s^2/2 + s^3/3! + s^4/4! ...) + 1/2 * (1 - s + s^2/2 - s^3/3! + s^4/4!`
- `= Î£{k = 0^âˆ} s^{2k}/(2k)! â‰¤ Î£ (s^2/2)^k/k! = e^{s^2/2}`
- `1/(2k)! â‰¤ 1/(2^k k!)`
- `ğ”¼[e^{sX}] / e^{st} â‰¤ e^{s^2/2}`
- `e^{-t^2/(2Ïƒ^2)} = e^{-t^2/2n}`
- `s = t/n`


INFORMATION THEORY
==================


Ex:
---
- data (book, DNA, binary file)
- what is the informational content?
- model:
	- `X1, ..., Xn` independent identically distributed random variables over a finite alphabet `A`
	- `âˆ€i âˆˆ â„•, âˆ€a âˆˆ A: p(a) = P[Xi = a]`
	- `p = (p(a)){a âˆˆ A}`
- `A^n` possible words
- `C_n âŠ† A^n` contains typical words
- the goal is to encode `C_n` with a small amount of bits
- `{Xn} = (X1, ..., Xn)` 								`{Xn} = \vec{Xn}`
- `L(n, Îµ) = min{l: âˆƒ C_n âŠ‚ A^n: |Cn| â‰¤ 2^l}`
	- `|Cn| â‰¤ 2^l` we can encode using `l` bits
	- `P[{Xn} âˆˆ Cn] â‰¥ 1 - Îµ`


Df: Entropy function
--------------------
- `H(p) = Î£{a âˆˆ A} -p(a) * log2(p(a))`


Th: Source coding theorem (Shannon: 1948)
-----------------------------------------
- `âˆ€Îµ > 0: lim{n -> âˆ} L(n, Îµ)/n = H(p)`
- entropy of `p` or entropy of `Xi`

### Proof:
1. `â‰¤`
	+ `Yi = log2(1/p(Xi))`
	+ `ğ”¼[Yi] = Î£{a âˆˆ A} P[Xi = a] ğ”¼[Yi | Xi = a] = Î£{a âˆˆ A} -p(a) * log2(p(a))`
	+ `Y1, Y2, ...` infinite sequence of independent, identically distributed random variables
	+ `A` is finite  `=>   var[Yi]` is finite  `=>`  we can use the weak law of large numbers
	
	+ `Sn = Î£ Yi / n`
	+ by the weak law: `P[|Sn - ğ”¼[Sn]| > Î´| -> 0`
	+ `ğ”¼[Sn] = ğ”¼[Yi] = H(p)`
	+ `< Î´` if `n` is large
	+ `Sn = -1/n Î£{n = 1^n} log2(p(Xi)) = -1/n log2(Ï€ p(Xi)) =independence= -1/n log2(p^*({Xn}))` for `p^*` probability on `n`-tuples
	+ `p^*({Xn}) = 2^{-n * Sn}`
	+ `p^*({Xn}) â‰ 2^{-n * Sn}` with high probability
	+ `Bn = \{w âˆˆ A^n: 2^{-n * (H(p) + Î´)} â‰¤ p^*(w) â‰¤ 2^{-n * (H(p) - Î´)}\}`
	+ if `0 < Î´ < Îµ`, then we can use `Cn = Bn`
		* `P[Bn] â‰¥ 1 - Î´ â‰¥ 1 - Îµ`
		* `1 = Î£{w âˆˆ A^n} p^*(w) â‰¥ Î£{w âˆˆ Bn} p^*(w) â‰¥ |Bn| * 2^{-n(H(p) + Î´)}`
		* `|Bn| â‰¤ 2^{n(H(p) + Î´)}`
		* `L(n, Îµ) â‰¤ n(H(p) + Î´)`