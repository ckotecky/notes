2022-12-20
lecture #12
-----------



Df: Permutation test
--------------------
- random variables `{x} = x_1, ..., x_n` and `{y} = y_1, ..., y_m` that we want to compare
- `H_0 =` both sets are from the same (arbitrary) distribution

+ `f({x}, {y})` arbitrary function, usually `(x_1 + ... + x_n) / n - (y_1 + ... + y_m) / m`
+ `f_0 ‚àà ‚Ñù` the value of `f({x}, {y})`
+ `{z} = ({x}, {y})`
+ `ùìï := \{ f(œÄ(z)) : œÄ ‚àà S_{m + n} \}` multiset, where `œÄ(z) = z_{œÄ(1)}, ..., z_{œÄ(m+n)}`

- `p`-value is the fraction of `ùìï` that is `‚â• f_0`
- reject `H_0` if `p`-value `< Œ±`

+ assuming `H_0`, all values in `ùìï` are equally likely to be the value of `f_0 => P[`error of type 1.`] < Œ±`

- improvements: if `(m+n)!` is too big
	- we don't want to evaluate `f`
	- instead we sample `œÄ_1, ..., œÄ_b ‚àà S_{n+m}`
	- `ùìï^* = \{ f(œÄ_i({z})) : i ‚àà [b] \}`
	- `p`-value = `|\{ i : f(œÄ_i(z)) ‚â• f_0 \}| / b`


Df: Sign test
-------------
1. one sample test: `X_1, ..., X_n` continuous independent identically distributed random variables of unknown distribution
	1. median `Œº`
	2. mean `Œº` and symmetrical distribution
	
	- `H_0 := Œº = 0`
	- `Y_i = sgn(X_i) = \{`
		+ `1`
		+ `0`
	- assuming `H_0 => Y = Œ£ Yi ~ Bin(n, 1/2)`
		+ `P[Xi < 0] = P[Xi > 0] = 1/2`
		
	+ `Q`
	+ reject if `Y > y_{1 - Œ±/2}` or `Y < y_{a/2}`

2. pair test: `(X_1, Y_1), ..., (X_n, Y_n)`
	- can be numbers before and after treatment for example

	+ `H_0 :=` mean of `X =` mean of `Y`
	+ or if mean `(X - Y) = Z = 0`
		+ samples then become `(Z_1 = (X_1 - Y_1, ..., X_n - Y_n)` and we get one sample test


Ex: Wilcoxon signed rank test
-----------------------------
|                  | -0.3  | +0.4 | -0.9 | -0.5 | +0.2 | -0.2 | -0.3 | +0.2 | -0.6 | -0.1 |
|------------------|-------|---|---|---|---|---|---|---|---|---|
| sign text        |   0   | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 |
| rank             | 5 - 6 | 7  | 10 | 8 | 2 - 4 | 2 - 4 | 5 - 6 | 2 - 4 | 9 | 1 |
| average rank `r` |  5.5  | 7 | 10 | 8 | 3 | 3 | 3.5 | 3 | 9 | 1 |

+ `T^+ = 7 + 3 + 3 = 13`
+ `T^- = 10 + 8 + 3 + 9 + 1 = 42?`

- sign test: `P[Bin(10, 1/2) ‚â§ 3] < Œ±/2`
- but some negative numbers are quite large
- we want a test that notices how large values are without assuming a distribution

+ in reality, sampling from continuous distribution would not result in repeated numbers


Df: Wilcoxon signed rank test
-----------------------------
- data `x_1, ..., x_n` from continuous distribution with median `Œº`
- we assume the distribution is symmetric around `Œº:  f_X(Œº - t) = f_X(Œº + t)`

+ `W = T = Œ£_{i ‚àà [n]} r_i * sgn(x_i) = T^+ - T^-`
+ `T^+ = Œ£_{i ‚àà [n]: x_i > 0} r_i`
+ `T^- = Œ£_{i ‚àà [n]: x_i < 0} r_i`
+ sum `T^+ + T^- = n(n+1) / 2` is not affected by the data

- `H_0 := Œº = 0`
- reject `H_0` if `T` is too large or too small

+ assuming `H_0` and symmetry `=>` ranks `r ‚àà [n]` are equally likely to be in `T^+` or in `T^-`
+ `T` can have `2^n` values


Df: Mann-Whitney U-test
-----------------------
- two-sample test
- specific form of permutation test
- does not require addition
	- variables might be a scale that is hard to convert to numerical values but easy to compare
- `U = Œ£_{i ‚àà n} Œ£_{j ‚àà m} S(X_i, Y_j)`
- `S(X_i, Y_j) = \{`
	- `0` 		`X_i > Y_j`
	- `1/2` 	`X_i = Y_j`
	- `1` 		`X_i < Y_j`
	
+ `H_0 := P[X < Y] = P[X > Y]`


Ex: Simpson's paradox
---------------------
- success rate for two doctors
- one doctor is better in both specific cases but worse in average

|                    | doctor A | doctor B |
|--------------------|----------|----------|
| success rate       |   90 %   |   80 %   |
| ratio              | 90 / 100 | 80 / 100 |
|--------------------|----------|----------|
|                    |   99 %   |  100 %   |
| twisted ankle      | 89 / 90  | 10 / 10  |
|--------------------|----------|----------|
|                    |   10 %   |   77 %   |
| open heart surgery |  1 / 90  | 70 / 90  |


- caused by hidden **lurking variables**
- solutions:
	1. stratified testing
		- guess which hidden variables there might be
	2. controlled experiments
		- control everything except one variable we want to test
		- we don't always have the means
		

Ex: Experiment design
---------------------
- crop yield
- we compare crop type A against crop type B
- divide field in half
- the soil quality may not be uniform for example

+ when variables cannot be controlled, we randomize
+ divide the field into small pieces and for each we chose type A or B randomly
+ checkerboarding might work here but not generally
+ with good probability, the hidden variables will affect A and B by almost the same amount


Ex: Time dependency
-------------------
- `X_1, ..., X_n` measured data
- all tests assume independent identical distribution
- perhaps `ùîº[X_i]` depends on `i`
	1. a person doing something gets tired
	2. machine is getting warm
	3. different workers at different times

+ replace `X_i` by `Y_i = sgn(X_i - Œº)` for median `Œº`
+ compare with number of runes with uniformly random `+/-`