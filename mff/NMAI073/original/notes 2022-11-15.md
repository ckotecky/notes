2022-11-15
lecture #6
----------



Nt:
---
- `f{Θ|X}(𝛝|x) = fΘ(𝛝) * f{X|Θ}(x|𝛝) / ∫ (fΘ(𝛝') * f{X|Θ}(x|𝛝') d𝛝')`
- `f{Θ|X}(𝛝|x) = fΘ(𝛝) * f{X|Θ}(x|𝛝) / Σ (fΘ(𝛝') * f{X|Θ}(x|𝛝'))`

+ solved by numerical methods
	- sampling from distribution with PDF `fΘ` / PDF `pΘ`
	- MCMC (Markov chain Monte Carlo) method
		- Markov chain: created from a stationary distribution
		- Monte Carlo: randomized algorithm




CONDITIONAL EXPECTATION
=======================

- `𝔼[Y] = Σ{y ∈ Im(Y)} y * P[Y = y]`
- `𝔼[Y] = ∫{-∞, +∞} y * fY(y)`

+ `A ⊆ Ω, A ∈ 𝓙:`
- `𝔼[Y|A] = Σ y P[Y = y | A]`
- `𝔼[Y|A] = ∫ y f{Y|A}(y)`



Df: Conditional expectation
---------------------------
- `X,Y` random variables
- `x ∈ ℝ`
+ `g(x) := 𝔼[Y | X = x]`
	* `g(x):ℝ -> ℝ`
+ `𝔼[Y | X] = g(X)`

- `X:Ω -> g:ℝ -> ℝ`


Ex: Conditional expectation
---------------------------
- Coin with probability of Heads equal to `X ~ U(0,1)`
- `Y =` # of H in n tosses
- `{Y | X = x} ~ Bin(n, x)`
- `𝔼[Y | X = x] = nx`
- `𝔼[Y | X] = nX`


Ob: Law of iterated expectation (LIE)
-------------------------------------
- `𝔼[𝔼[Y | X]] = 𝔼[g(X)] =(LOTUS)= Σ{x ∈ Im(X)} g(x) * P[X = x] = Σ{x ∈ Im(X)} P[X = x] * 𝔼[Y | X = x] =(Law of total 𝔼)= 𝔼[Y]`
	+ assuming X is discrete
- holds if `𝔼[Y] < ∞`

- `𝔼[Y] = 𝔼[𝔼[Y|X]] = 𝔼[nX] = n𝔼[X]= n/2`


Ex:
---
- stick of length l
- break the stick at a random point `X ~ U(0,l)`
- assuming `X = x`, we break the left part at point `Y ~ U(0,x)`
+ `𝔼[Y] = 𝔼[𝔼[Y|X]] = 𝔼[X/2] = 𝔼[X]/2 = l/4`
	* `𝔼[Y|X] = g(X)`
	* `g(x) = 𝔼[Y | X = x] ~ U(0,x)`


Ex:
---
| group |    score	 |  `|A|`  |  `{{Y}}`   |
|   `X`   | 	   `Y`     |       |          |
|-------|------------|-------|----------|
|   1   |10,15,20,17 |   4   | 	 175	|
|   2   |   5, 30    |   2   |   175    |
|  ...  |  100, 200  |   2   |   150	|
|   `k`   |    ...     |  ...  |			|

- set of students `|Ω| = 1`
- `Ax = {ω ∈ Ω: X(ω) = x}`
- `nx = |Ax|`
- `𝔼[Y] = Σ{ω ∈ Ω} Y(ω) * 1/n = Σ{x = 1, k} (Σ{ω ∈ Ax} Y(ω) * 1/nx) * nx/n`
	+ `Σ{ω ∈ Ax} Y(ω) * 1/nx = 𝔼[Y | X = x]`
	+ `Σ{ω ∈ Ax} Y(ω) * 1/nx * P[X = x]`
- `= Σ{x = 1, k} Σ{ω ∈ Ax} Y(ω) * 1/nx * P[X = x]`
- `𝔼[Y]` estimate of `Y`
- `{{Y}} = 𝔼[Y | X]` 														`{{Y}} = \hat{Y}`
	+ `{{Y}}:Ω -> ℝ`
	+ `X` is some observed data
	+ `Y` quantity of interest
	+ `{{Y}}` is called **estimator** (function)
- `{{{Y}}} `				 												`{{{Y}}} = \tilde{Y}`
	+ `{{{Y}}} = {{Y}} - Y`
	+ **error of estimation**
- `𝔼[{{{Y}}} | X] = 𝔼[{{Y}} - Y | X] = 𝔼[{{Y}} | X] = 𝔼[Y | X]`
	+ `𝔼[{{Y}} | X] = 𝔼[𝔼[Y|X] | X] = {{Y}} = 𝔼[Y | X]`
	+ `𝔼[{{{Y}}}] = 0`
	+ bias of `{{Y}}` is 0 (by LIE)

+ `𝔼[{{{Y}}} * {{Y}}] =?= 0 = 𝔼[{{{Y}}}] * 𝔼[{{Y}}]`
	- `=> cov[{{{Y}}}, {{Y}}] = 0`
	- `𝔼[𝔼[{{{Y}}} * {{Y}} | X]] =!= 𝔼[{{{Y}}} * 𝔼[{{Y}} | X]] = 𝔼[0] = 0`
	
+ `Y = {{Y}} - {{{Y}}}`
+ `var[Y] = var[{{Y}}] + var[{{{Y}}}] - 2cov[{{Y}}, {{{Y}}}]`
+ `{{Y}}, {{{Y}}}` are statistically independent - their covariance is 0
+ `var[{{{Y}}}] = 𝔼[{{{Y}}}^2] =LIE= 𝔼[𝔼[{{Y}}^2 | X]]`
+ `𝔼[{{{Y}}}^2 | X] = 𝔼[(Y - {{Y}})^2 | X] = 𝔼[(Y - 𝔼[Y|X])^2 | X] = var[Y | X]` **conditional variance**
	* `var[Y | X] = h(X)`
	* `h(x) = var[Y | X = x]`
	* `var[Y] = 𝔼[var[Y | X]] + var[𝔼[Y | X]]` **law of iterated variance /  Eve's rule** 
		- `var[Y | X]` = variance within the group
		- `var[𝔼[Y | X]]` = intergroup variance



LMS `<=>` conditional expectations
----------------------------------
- given random variable `Y`, what value of `y` minimizes `𝔼[Y - y]^2`
- `𝔼[Y - y]^2 = 𝔼[Y^2] - 2y𝔼[Y] + y^2 = f(y)`
- `f'(y) = -2𝔼[Y] + 2y = 0   =>   y = 𝔼[Y]`

+ `∀x`: find `y = y(x): 𝔼[(Y - y(x))^2 | X = x]` is minimized
	- same calculation restricted to a smaller probability space  `=>  y(x) = 𝔼[Y | X = x]`
	- our *best* estimator is `{{Y}} = 𝔼[Y|X]`
		+ best in the LMS sense