2022-11-01
lecture #4
----------


Rp: 2-SAT
---------
- find unsatisfied clause
- (* ) change the random variable in it
- `Xt =` # of correctly assigned variables at time t of the algorithm
- `P[no success in time = 2mn^2] < 1/2^m`
- random walk on a markov chain
	+ path with `1/2` chance of going left/right
	+ loop with `p = 1` at the end (expression is satisfied)


Alg: 3-SAT
----------
- `ğ”¼[t to get from 0 to n] ~ 2^n`
- same chain but chances `1/3` vs `2/3` - we move left on average
+ randomly true / false for each variable
	* `X0 ~ Bin(n, 1/2)`
	* `ğ”¼[X0] = n/2`

1. improvement:
	1. initialize variables at random
	2. repeat (* ) up to `n/2` times
	3. stop if success
	4. fail

- `p = P[success of one attempt] â‰¥ P[X0 â‰¥ n/2] * P[sucess of one attempt | X0 â‰¥ n/2] â‰¥ 1/2 * 3^{-n/2}`

2. Loop: repeat attempt until success

- `T` random variable = # of repetitions of one attempt
- `T ~ Geom(p)`
- `ğ”¼[T] = 1/p â‰¤ 2 * 3^{n/2} ~ 2 * 1.6^n << 2^n`
+ `a = 2 * 3^{n/2}`
+ `P[T > 2 * a] â‰¤ ğ”¼[T]/a < 1/2`

3. Loop improvement: repeat for `â‰¤ 4 * 3^{n/2}` steps
		- If we don't succeed, we say "time-out"

- `P[time out] â‰¤ 1/2`

5. Final algorithm: Repeat loop for `â‰¤ m` times:
	+ If we don't succeed, we say "fail"
	
- `P[fail] â‰¤ (1/2)^m`
- running time `~ m * 4 * 3^{n/2} -----> (4/3)^n` with better analysis
	+ there will be a link on the lecture page with the better analysis




BAYESIAN STATISTICS
===================

- what is probability?
	+ mathematical concept
		- axioms
		- examples: (# good) / (# all)
		- theorems...
		* probabilistic method
			- to show `A â‰  âˆ…`, we show `P[A] > 0`
			- low bounds for Ramsay number
	+ description of real world
		- Q: does nature play dice?
			+ yes if QM is correct - true randomness
			+ imprecise measurements - pseudo randomness
		1. frequentist's approach
			- probability = (# good) / (# all)
			- probability is the limit of frequencies
		2. bayesian approach
			- subjective probability
			- how are you willing to bet?
			- `Î©` = all possible universes
			- `Ï‰` = our universe

			
Bayesian statistics
------------------- 															
																				PMF 	PDF
- `Î˜` 				random variable describing some quantity of interest 		`p_Î˜`  	`f_Î˜`
- `X (X1, ..., Xn)`	measurement, also random variables 							`p_X` 	`f_X`

- comparison with frequentist approach:
	+ `Î˜` as random variable does not exist
	+ we have `ğœ—` unknown fixed parameter 


Th: Bayes Theorem
-----------------
- `P[A], P[B] > 0`
- `P[B|A] = P[B] * P[A|B] / P[A]`

+ `A is [X = x]` 		data / measurement
+ `B is [Î˜ = ğœ—]` 		parameter

- `P[Î˜ = ğœ— | X = x] = P[Î˜ = ğœ—] * P[X = x | Î˜ = ğœ—] / P[X = x]`

+ value of `Î˜` is something we care about
+ `P[Î˜ = ğœ—]` is prior probability
+ `P[Î˜ = ğœ— | X = x]` is posterior probability - probability after measurement
+ `P[X = x | Î˜ = ğœ—]` is a model of the world - likelihood


Th: Bayes theorem using PMF
---------------------------
- `p{Î˜|X}(ğœ—|x) = p_Î˜(ğœ—) * p_{X|Î˜}(x|ğœ—) / (Î£{ğœ—'} p_Î˜(ğœ—') * p_{X|Î˜}(x|ğœ—'))`
	+ `= c * p_Î˜(ğœ—) * p_{X|Î˜}(x|ğœ—)`  	for `c` some constant


- what do we want? (from the frequentist approach)
	+ point estimates for `Î˜`
		* tricky
		* two approaches
		1. MAP (maximum aposterior probability)
			- `{{ğœ—}} = argmax{ğœ—} p{Î˜|X}(ğœ—|x)	`												`{{ğœ—}} = \hat{ğœ—}`
			- if `X = x` what is the most likely value?
		2. LMS (least mean square)
			- `{{ğœ—}} = argmin{ğœ—} ğ”¼[(Î˜ - ğœ—)^2 | X = x] = ğ”¼[Î˜ | X = x]`
	+ interval estimates for `Î˜`
		* given `X`, provide interval `[a,b]`
			- `(a = a(X), b = b(X))`
			1. `P[a(x) < Î˜ < b(x) | X = x] > 1 - Î±`
			2. or we might want `P[Î˜ < a(x) | X = x] = P[Î˜ > b(x) | X = x] = Î± / 2`
	+ hypothesis testing