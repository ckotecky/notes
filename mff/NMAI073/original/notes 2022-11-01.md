2022-11-01
lecture #4
----------


Rp: 2-SAT
---------
- find unsatisfied clause
- (* ) change the random variable in it
- `Xt =` # of correctly assigned variables at time t of the algorithm
- `P[no success in time = 2mn^2] < 1/2^m`
- random walk on a markov chain
	+ path with `1/2` chance of going left/right
	+ loop with `p = 1` at the end (expression is satisfied)


Alg: 3-SAT
----------
- `𝔼[t to get from 0 to n] ~ 2^n`
- same chain but chances `1/3` vs `2/3` - we move left on average
+ randomly true / false for each variable
	* `X0 ~ Bin(n, 1/2)`
	* `𝔼[X0] = n/2`

1. improvement:
	1. initialize variables at random
	2. repeat (* ) up to `n/2` times
	3. stop if success
	4. fail

- `p = P[success of one attempt] ≥ P[X0 ≥ n/2] * P[sucess of one attempt | X0 ≥ n/2] ≥ 1/2 * 3^{-n/2}`

2. Loop: repeat attempt until success

- `T` random variable = # of repetitions of one attempt
- `T ~ Geom(p)`
- `𝔼[T] = 1/p ≤ 2 * 3^{n/2} ~ 2 * 1.6^n << 2^n`
+ `a = 2 * 3^{n/2}`
+ `P[T > 2 * a] ≤ 𝔼[T]/a < 1/2`

3. Loop improvement: repeat for `≤ 4 * 3^{n/2}` steps
		- If we don't succeed, we say "time-out"

- `P[time out] ≤ 1/2`

5. Final algorithm: Repeat loop for `≤ m` times:
	+ If we don't succeed, we say "fail"
	
- `P[fail] ≤ (1/2)^m`
- running time `~ m * 4 * 3^{n/2} -----> (4/3)^n` with better analysis
	+ there will be a link on the lecture page with the better analysis




BAYESIAN STATISTICS
===================

- what is probability?
	+ mathematical concept
		- axioms
		- examples: (# good) / (# all)
		- theorems...
		* probabilistic method
			- to show `A ≠ ∅`, we show `P[A] > 0`
			- low bounds for Ramsay number
	+ description of real world
		- Q: does nature play dice?
			+ yes if QM is correct - true randomness
			+ imprecise measurements - pseudo randomness
		1. frequentist's approach
			- probability = (# good) / (# all)
			- probability is the limit of frequencies
		2. bayesian approach
			- subjective probability
			- how are you willing to bet?
			- `Ω` = all possible universes
			- `ω` = our universe

			
Bayesian statistics
------------------- 															
																				PMF 	PDF
- `Θ` 				random variable describing some quantity of interest 		`p_Θ`  	`f_Θ`
- `X (X1, ..., Xn)`	measurement, also random variables 							`p_X` 	`f_X`

- comparison with frequentist approach:
	+ `Θ` as random variable does not exist
	+ we have `𝜗` unknown fixed parameter 


Th: Bayes Theorem
-----------------
- `P[A], P[B] > 0`
- `P[B|A] = P[B] * P[A|B] / P[A]`

+ `A is [X = x]` 		data / measurement
+ `B is [Θ = 𝜗]` 		parameter

- `P[Θ = 𝜗 | X = x] = P[Θ = 𝜗] * P[X = x | Θ = 𝜗] / P[X = x]`

+ value of `Θ` is something we care about
+ `P[Θ = 𝜗]` is prior probability
+ `P[Θ = 𝜗 | X = x]` is posterior probability - probability after measurement
+ `P[X = x | Θ = 𝜗]` is a model of the world - likelihood


Th: Bayes theorem using PMF
---------------------------
- `p{Θ|X}(𝜗|x) = p_Θ(𝜗) * p_{X|Θ}(x|𝜗) / (Σ{𝜗'} p_Θ(𝜗') * p_{X|Θ}(x|𝜗'))`
	+ `= c * p_Θ(𝜗) * p_{X|Θ}(x|𝜗)`  	for `c` some constant


- what do we want? (from the frequentist approach)
	+ point estimates for `Θ`
		* tricky
		* two approaches
		1. MAP (maximum aposterior probability)
			- `{{𝜗}} = argmax{𝜗} p{Θ|X}(𝜗|x)	`												`{{𝜗}} = \hat{𝜗}`
			- if `X = x` what is the most likely value?
		2. LMS (least mean square)
			- `{{𝜗}} = argmin{𝜗} 𝔼[(Θ - 𝜗)^2 | X = x] = 𝔼[Θ | X = x]`
	+ interval estimates for `Θ`
		* given `X`, provide interval `[a,b]`
			- `(a = a(X), b = b(X))`
			1. `P[a(x) < Θ < b(x) | X = x] > 1 - α`
			2. or we might want `P[Θ < a(x) | X = x] = P[Θ > b(x) | X = x] = α / 2`
	+ hypothesis testing